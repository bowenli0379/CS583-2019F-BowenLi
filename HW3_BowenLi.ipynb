{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc9voBBHmcRg"
   },
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Bowen Li]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf2liZuYmcRj"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iozy9850mcRj"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-W2beoztmcRk"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "TfkY8qMmmcRl",
    "outputId": "0eb65bb7-4120-4229-a103-10ddb71e3f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGilFqFSmcRo"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "havWS8hQmcRp",
    "outputId": "17f6f3d9-3452-4f26-d9ee-8cc4733a3b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    res = []\n",
    "    for ys in y:\n",
    "        vec = [0]*num_class\n",
    "        vec[ys[0]] = 1\n",
    "        res.append(vec)\n",
    "    return numpy.asarray(res)\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4gkddaxmcRr"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC_jlld_mcRs"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "d46ENLG_mcRs",
    "outputId": "0e119b05-d8cd-4bc8-c181-6d85a4f04b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAlsOdL9mcRu"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQ785BsBmcRv"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCH7kYW5mcRv"
   },
   "source": [
    "## 2.1 With more layers, dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "KJaqPxb4mcRw",
    "outputId": "364f2b8c-f331-4a73-b3fb-bd977c447459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,251,626\n",
      "Trainable params: 1,251,242\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXEUBV3fmcRy"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "opt = optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ozkvVDgEmcRz",
    "outputId": "db8ff9f6-0b31-47cc-8579-11c8a09a3a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 21s 528us/step - loss: 1.8481 - acc: 0.3358 - val_loss: 1.4405 - val_acc: 0.4764\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 1.5293 - acc: 0.4482 - val_loss: 1.2722 - val_acc: 0.5368\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 1.3849 - acc: 0.5082 - val_loss: 1.1800 - val_acc: 0.5748\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 1.2911 - acc: 0.5433 - val_loss: 1.1140 - val_acc: 0.6028\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 1.2058 - acc: 0.5721 - val_loss: 1.1227 - val_acc: 0.5987\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 1.1280 - acc: 0.6040 - val_loss: 0.9806 - val_acc: 0.6487\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 1.0702 - acc: 0.6235 - val_loss: 0.9010 - val_acc: 0.6815\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 1.0235 - acc: 0.6385 - val_loss: 0.9084 - val_acc: 0.6810\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.9843 - acc: 0.6537 - val_loss: 0.9263 - val_acc: 0.6663\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.9468 - acc: 0.6704 - val_loss: 0.8516 - val_acc: 0.7032\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.9196 - acc: 0.6769 - val_loss: 0.8271 - val_acc: 0.7111\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.8927 - acc: 0.6879 - val_loss: 0.7980 - val_acc: 0.7247\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 21s 527us/step - loss: 0.8658 - acc: 0.6955 - val_loss: 0.8711 - val_acc: 0.6941\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.8419 - acc: 0.7062 - val_loss: 0.7982 - val_acc: 0.7214\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.8217 - acc: 0.7130 - val_loss: 0.7395 - val_acc: 0.7396\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.8006 - acc: 0.7217 - val_loss: 0.7515 - val_acc: 0.7377\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.7881 - acc: 0.7270 - val_loss: 0.7289 - val_acc: 0.7465\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.7756 - acc: 0.7331 - val_loss: 0.7285 - val_acc: 0.7443\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.7583 - acc: 0.7369 - val_loss: 0.7410 - val_acc: 0.7451\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.7489 - acc: 0.7413 - val_loss: 0.7228 - val_acc: 0.7500\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.7279 - acc: 0.7476 - val_loss: 0.7375 - val_acc: 0.7433\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.7238 - acc: 0.7504 - val_loss: 0.7083 - val_acc: 0.7521\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.7127 - acc: 0.7524 - val_loss: 0.7158 - val_acc: 0.7545\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.7026 - acc: 0.7577 - val_loss: 0.6789 - val_acc: 0.7656\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6896 - acc: 0.7610 - val_loss: 0.6785 - val_acc: 0.7619\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.6900 - acc: 0.7648 - val_loss: 0.7182 - val_acc: 0.7473\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.6817 - acc: 0.7680 - val_loss: 0.6599 - val_acc: 0.7734\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 21s 520us/step - loss: 0.6733 - acc: 0.7692 - val_loss: 0.6879 - val_acc: 0.7690\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6657 - acc: 0.7718 - val_loss: 0.6676 - val_acc: 0.7723\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6604 - acc: 0.7739 - val_loss: 0.6728 - val_acc: 0.7680\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6559 - acc: 0.7740 - val_loss: 0.6910 - val_acc: 0.7656\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6504 - acc: 0.7785 - val_loss: 0.6403 - val_acc: 0.7769\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.6496 - acc: 0.7764 - val_loss: 0.6693 - val_acc: 0.7702\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.6470 - acc: 0.7773 - val_loss: 0.6746 - val_acc: 0.7677\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.6361 - acc: 0.7838 - val_loss: 0.6446 - val_acc: 0.7792\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6339 - acc: 0.7836 - val_loss: 0.6703 - val_acc: 0.7746\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.6316 - acc: 0.7863 - val_loss: 0.6831 - val_acc: 0.7665\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.6335 - acc: 0.7833 - val_loss: 0.6649 - val_acc: 0.7757\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.6256 - acc: 0.7886 - val_loss: 0.6579 - val_acc: 0.7769\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.6207 - acc: 0.7891 - val_loss: 0.6530 - val_acc: 0.7801\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6191 - acc: 0.7900 - val_loss: 0.6202 - val_acc: 0.7896\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.6144 - acc: 0.7927 - val_loss: 0.6544 - val_acc: 0.7765\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 21s 525us/step - loss: 0.6155 - acc: 0.7935 - val_loss: 0.6402 - val_acc: 0.7858\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6023 - acc: 0.7948 - val_loss: 0.6555 - val_acc: 0.7804\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.6077 - acc: 0.7957 - val_loss: 0.6719 - val_acc: 0.7752\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.6003 - acc: 0.7973 - val_loss: 0.6478 - val_acc: 0.7791\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.6020 - acc: 0.7985 - val_loss: 0.6630 - val_acc: 0.7727\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.5999 - acc: 0.7980 - val_loss: 0.6414 - val_acc: 0.7836\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5957 - acc: 0.7976 - val_loss: 0.6648 - val_acc: 0.7759\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.5957 - acc: 0.8000 - val_loss: 0.6787 - val_acc: 0.7738\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6001 - acc: 0.8004 - val_loss: 0.6667 - val_acc: 0.7804\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.5963 - acc: 0.8002 - val_loss: 0.6427 - val_acc: 0.7846\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5911 - acc: 0.8034 - val_loss: 0.6716 - val_acc: 0.7748\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5858 - acc: 0.8033 - val_loss: 0.6710 - val_acc: 0.7870\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5964 - acc: 0.8040 - val_loss: 0.6506 - val_acc: 0.7864\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5939 - acc: 0.8002 - val_loss: 0.6234 - val_acc: 0.7946\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.5857 - acc: 0.8032 - val_loss: 0.6352 - val_acc: 0.7846\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 21s 527us/step - loss: 0.5833 - acc: 0.8057 - val_loss: 0.6255 - val_acc: 0.7931\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5785 - acc: 0.8074 - val_loss: 0.6289 - val_acc: 0.7914\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 21s 520us/step - loss: 0.5799 - acc: 0.8058 - val_loss: 0.6491 - val_acc: 0.7850\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5863 - acc: 0.8049 - val_loss: 0.6308 - val_acc: 0.7936\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 21s 522us/step - loss: 0.5864 - acc: 0.8046 - val_loss: 0.6594 - val_acc: 0.7886\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5857 - acc: 0.8062 - val_loss: 0.6295 - val_acc: 0.7925\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5822 - acc: 0.8072 - val_loss: 0.7206 - val_acc: 0.7679\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5837 - acc: 0.8081 - val_loss: 0.6349 - val_acc: 0.7893\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.5812 - acc: 0.8082 - val_loss: 0.6423 - val_acc: 0.7906\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5721 - acc: 0.8093 - val_loss: 0.6849 - val_acc: 0.7709\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.5780 - acc: 0.8097 - val_loss: 0.6475 - val_acc: 0.7861\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5786 - acc: 0.8092 - val_loss: 0.6734 - val_acc: 0.7739\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5765 - acc: 0.8103 - val_loss: 0.6235 - val_acc: 0.7942\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5829 - acc: 0.8095 - val_loss: 0.6596 - val_acc: 0.7855\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 21s 523us/step - loss: 0.5805 - acc: 0.8095 - val_loss: 0.7526 - val_acc: 0.7632\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 21s 523us/step - loss: 0.5765 - acc: 0.8105 - val_loss: 0.6802 - val_acc: 0.7774\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 21s 524us/step - loss: 0.5800 - acc: 0.8093 - val_loss: 0.6350 - val_acc: 0.7919\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.5748 - acc: 0.8089 - val_loss: 0.6407 - val_acc: 0.7926\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.5733 - acc: 0.8100 - val_loss: 0.6960 - val_acc: 0.7690\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5779 - acc: 0.8117 - val_loss: 0.7309 - val_acc: 0.7601\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.5738 - acc: 0.8117 - val_loss: 0.6888 - val_acc: 0.7881\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5801 - acc: 0.8115 - val_loss: 0.6780 - val_acc: 0.7793\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5732 - acc: 0.8104 - val_loss: 0.6749 - val_acc: 0.7766\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5774 - acc: 0.8119 - val_loss: 0.6638 - val_acc: 0.7890\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5767 - acc: 0.8119 - val_loss: 0.6705 - val_acc: 0.7797\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5826 - acc: 0.8095 - val_loss: 0.6994 - val_acc: 0.7733\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 21s 521us/step - loss: 0.5708 - acc: 0.8139 - val_loss: 0.6567 - val_acc: 0.7942\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5774 - acc: 0.8129 - val_loss: 0.6742 - val_acc: 0.7788\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5746 - acc: 0.8119 - val_loss: 0.6521 - val_acc: 0.7898\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5791 - acc: 0.8103 - val_loss: 0.6755 - val_acc: 0.7771\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 21s 520us/step - loss: 0.5834 - acc: 0.8125 - val_loss: 0.6679 - val_acc: 0.7846\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5798 - acc: 0.8112 - val_loss: 0.6925 - val_acc: 0.7723\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.5859 - acc: 0.8082 - val_loss: 0.6697 - val_acc: 0.7830\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.5857 - acc: 0.8079 - val_loss: 0.6746 - val_acc: 0.7839\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5801 - acc: 0.8121 - val_loss: 0.6268 - val_acc: 0.7985\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.5846 - acc: 0.8093 - val_loss: 0.6942 - val_acc: 0.7800\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5862 - acc: 0.8097 - val_loss: 0.6601 - val_acc: 0.7899\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.5912 - acc: 0.8093 - val_loss: 0.6947 - val_acc: 0.7709\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5812 - acc: 0.8107 - val_loss: 0.6655 - val_acc: 0.7815\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5803 - acc: 0.8109 - val_loss: 0.6843 - val_acc: 0.7790\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5820 - acc: 0.8119 - val_loss: 0.6981 - val_acc: 0.7768\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5882 - acc: 0.8103 - val_loss: 0.6990 - val_acc: 0.7745\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5881 - acc: 0.8095 - val_loss: 0.6790 - val_acc: 0.7834\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "z7hHuIZ2yUQf",
    "outputId": "64eee51d-3d48-433f-d726-3c1bc5282df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 7s 173us/step\n",
      "Training loss = 0.456110947573185\n",
      "Training accuracy = 0.874975\n",
      "10000/10000 [==============================] - 2s 174us/step\n",
      "Validation loss = 0.6789758045673371\n",
      "Validation accuracy = 0.7834\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc1 = model.evaluate(x_tr, y_tr)\n",
    "print('Training loss = ' + str(loss_and_acc1[0]))\n",
    "print('Training accuracy = ' + str(loss_and_acc1[1]))\n",
    "\n",
    "loss_and_acc2 = model.evaluate(x_val, y_val)\n",
    "print('Validation loss = ' + str(loss_and_acc2[0]))\n",
    "print('Validation accuracy = ' + str(loss_and_acc2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "AvAe7NSdmcR1",
    "outputId": "2adb7150-8f10-4b63-8573-74f5339cbfec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmDhngcAYY+KcBQJjjIlzFgiMMSbO/T99BSZdr5NjrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWZwWUf6mcR5"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QgzrXvHmcR6"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Zvdv7coamcR8",
    "outputId": "83fd69d4-4d9c-461c-eccf-8e531d125ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 25s 497us/step - loss: 0.6900 - acc: 0.7790\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.6705 - acc: 0.7843\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6676 - acc: 0.7855\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.6696 - acc: 0.7851\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6574 - acc: 0.7879\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6608 - acc: 0.7879\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6592 - acc: 0.7878\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.6652 - acc: 0.7877\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6594 - acc: 0.7881\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6647 - acc: 0.7870\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6574 - acc: 0.7871\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6573 - acc: 0.7886\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.6564 - acc: 0.7874\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6669 - acc: 0.7867\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6631 - acc: 0.7850\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.6603 - acc: 0.7866\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6609 - acc: 0.7883\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6622 - acc: 0.7859\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6593 - acc: 0.7895\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6635 - acc: 0.7900\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6561 - acc: 0.7899\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6608 - acc: 0.7870\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.6637 - acc: 0.7870\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6623 - acc: 0.7903\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.6695 - acc: 0.7867\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.6663 - acc: 0.7863\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6653 - acc: 0.7870\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6685 - acc: 0.7872\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6603 - acc: 0.7885\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6711 - acc: 0.7861\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6767 - acc: 0.7840\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6652 - acc: 0.7875\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6594 - acc: 0.7872\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.6732 - acc: 0.7858\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6699 - acc: 0.7849\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6642 - acc: 0.7914\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6700 - acc: 0.7869\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6714 - acc: 0.7872\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6820 - acc: 0.7829\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6765 - acc: 0.7848\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6731 - acc: 0.7875\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6717 - acc: 0.7855\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6779 - acc: 0.7846\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6758 - acc: 0.7875\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6840 - acc: 0.7832\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6821 - acc: 0.7834\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.6745 - acc: 0.7853\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6860 - acc: 0.7822\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.6794 - acc: 0.7846\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6884 - acc: 0.7850\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.6826 - acc: 0.7846\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6784 - acc: 0.7835\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6864 - acc: 0.7851\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6862 - acc: 0.7847\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6923 - acc: 0.7835\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6810 - acc: 0.7847\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6903 - acc: 0.7815\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6928 - acc: 0.7828\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6899 - acc: 0.7831\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6871 - acc: 0.7820\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6914 - acc: 0.7832\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.6926 - acc: 0.7850\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6925 - acc: 0.7819\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.7038 - acc: 0.7808\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.7052 - acc: 0.7782\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6923 - acc: 0.7833\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6980 - acc: 0.7814\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6957 - acc: 0.7825\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6972 - acc: 0.7820\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7025 - acc: 0.7812\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7000 - acc: 0.7808\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7057 - acc: 0.7808\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7011 - acc: 0.7775\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7067 - acc: 0.7790\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7039 - acc: 0.7803\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7001 - acc: 0.7811\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.7099 - acc: 0.7797\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.7030 - acc: 0.7794\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7033 - acc: 0.7808\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7057 - acc: 0.7811\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.7022 - acc: 0.7839\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7099 - acc: 0.7799\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7081 - acc: 0.7779\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.7150 - acc: 0.7793\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.7095 - acc: 0.7794\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7037 - acc: 0.7800\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.7170 - acc: 0.7777\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7102 - acc: 0.7772\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7129 - acc: 0.7762\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7051 - acc: 0.7809\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.7072 - acc: 0.7804\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7061 - acc: 0.7771\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.7082 - acc: 0.7781\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.7205 - acc: 0.7760\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7087 - acc: 0.7783\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7010 - acc: 0.7798\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7170 - acc: 0.7771\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7146 - acc: 0.7767\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.7097 - acc: 0.7805\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7161 - acc: 0.7793\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGFqNTcNmcR-"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JyX2dCCHmcR_",
    "outputId": "f756674a-ac25-4c49-cdf6-3691d7f6ebda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 174us/step\n",
      "Test loss = 0.7383095815658569\n",
      "Test accuracy = 0.761\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc3 = model.evaluate(x_test, y_test_vec)\n",
    "print('Test loss = ' + str(loss_and_acc3[0]))\n",
    "print('Test accuracy = ' + str(loss_and_acc3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Jt4x9qQmcSB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HW3_BowenLi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
