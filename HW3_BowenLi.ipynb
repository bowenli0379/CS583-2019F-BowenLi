{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc9voBBHmcRg"
   },
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Bowen Li]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pf2liZuYmcRj"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iozy9850mcRj"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-W2beoztmcRk"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "TfkY8qMmmcRl",
    "outputId": "118aea04-78c7-4cc6-bcd7-0cf09861fecc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGilFqFSmcRo"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "havWS8hQmcRp",
    "outputId": "0115fe8a-2694-4d28-97f5-97dccecc45d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    res = []\n",
    "    for ys in y:\n",
    "        vec = [0]*num_class\n",
    "        vec[ys[0]] = 1\n",
    "        res.append(vec)\n",
    "    return numpy.asarray(res)\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4gkddaxmcRr"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC_jlld_mcRs"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "d46ENLG_mcRs",
    "outputId": "10d5c3a7-8d4f-4121-9ea7-c0d75c90601d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAlsOdL9mcRu"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQ785BsBmcRv"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCH7kYW5mcRv"
   },
   "source": [
    "## 2.1 With more layers, dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "KJaqPxb4mcRw",
    "outputId": "2c741e99-0438-4db0-f79a-cef23c7070e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,251,626\n",
      "Trainable params: 1,251,242\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXEUBV3fmcRy"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ozkvVDgEmcRz",
    "outputId": "8db53123-6e5a-4570-d4bb-ea01d4996492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 21s 525us/step - loss: 1.8316 - acc: 0.3387 - val_loss: 1.4092 - val_acc: 0.4851\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 1.5123 - acc: 0.4553 - val_loss: 1.3234 - val_acc: 0.5238\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 1.3856 - acc: 0.5043 - val_loss: 1.1752 - val_acc: 0.5849\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 1.2863 - acc: 0.5460 - val_loss: 1.1278 - val_acc: 0.6043\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 1.2006 - acc: 0.5733 - val_loss: 1.0925 - val_acc: 0.6121\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 1.1345 - acc: 0.5993 - val_loss: 1.0297 - val_acc: 0.6353\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 1.0723 - acc: 0.6204 - val_loss: 0.9787 - val_acc: 0.6548\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 1.0276 - acc: 0.6369 - val_loss: 0.9738 - val_acc: 0.6503\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.9807 - acc: 0.6552 - val_loss: 0.8661 - val_acc: 0.6938\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 21s 520us/step - loss: 0.9447 - acc: 0.6684 - val_loss: 0.8720 - val_acc: 0.6876\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.9049 - acc: 0.6845 - val_loss: 0.8200 - val_acc: 0.7152\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.8699 - acc: 0.6967 - val_loss: 0.7845 - val_acc: 0.7263\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.8487 - acc: 0.7036 - val_loss: 0.7866 - val_acc: 0.7222\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.8256 - acc: 0.7112 - val_loss: 0.8000 - val_acc: 0.7221\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.8022 - acc: 0.7206 - val_loss: 0.8026 - val_acc: 0.7164\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.7868 - acc: 0.7278 - val_loss: 0.7999 - val_acc: 0.7290\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.7698 - acc: 0.7344 - val_loss: 0.7102 - val_acc: 0.7557\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.7459 - acc: 0.7391 - val_loss: 0.7351 - val_acc: 0.7457\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.7396 - acc: 0.7463 - val_loss: 0.7378 - val_acc: 0.7406\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.7284 - acc: 0.7484 - val_loss: 0.7286 - val_acc: 0.7481\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.7148 - acc: 0.7522 - val_loss: 0.7386 - val_acc: 0.7408\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.7031 - acc: 0.7580 - val_loss: 0.7317 - val_acc: 0.7472\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6935 - acc: 0.7603 - val_loss: 0.7583 - val_acc: 0.7330\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6851 - acc: 0.7659 - val_loss: 0.6643 - val_acc: 0.7709\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.6805 - acc: 0.7688 - val_loss: 0.6838 - val_acc: 0.7640\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6650 - acc: 0.7725 - val_loss: 0.6548 - val_acc: 0.7723\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.6635 - acc: 0.7730 - val_loss: 0.6805 - val_acc: 0.7695\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6612 - acc: 0.7739 - val_loss: 0.6637 - val_acc: 0.7719\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6502 - acc: 0.7758 - val_loss: 0.6572 - val_acc: 0.7758\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.6458 - acc: 0.7817 - val_loss: 0.6799 - val_acc: 0.7718\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.6428 - acc: 0.7816 - val_loss: 0.6640 - val_acc: 0.7698\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.6385 - acc: 0.7852 - val_loss: 0.7831 - val_acc: 0.7280\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.6310 - acc: 0.7853 - val_loss: 0.6620 - val_acc: 0.7770\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 20s 510us/step - loss: 0.6314 - acc: 0.7853 - val_loss: 0.6496 - val_acc: 0.7804\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6265 - acc: 0.7888 - val_loss: 0.6851 - val_acc: 0.7654\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.6202 - acc: 0.7902 - val_loss: 0.7218 - val_acc: 0.7570\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.6195 - acc: 0.7918 - val_loss: 0.6942 - val_acc: 0.7652\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.6180 - acc: 0.7937 - val_loss: 0.6267 - val_acc: 0.7891\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 20s 505us/step - loss: 0.6093 - acc: 0.7949 - val_loss: 0.6565 - val_acc: 0.7802\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.6081 - acc: 0.7938 - val_loss: 0.6203 - val_acc: 0.7920\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 20s 503us/step - loss: 0.6127 - acc: 0.7942 - val_loss: 0.6599 - val_acc: 0.7837\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 21s 518us/step - loss: 0.6047 - acc: 0.7975 - val_loss: 0.6632 - val_acc: 0.7864\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6076 - acc: 0.7992 - val_loss: 0.6586 - val_acc: 0.7874\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.6036 - acc: 0.7962 - val_loss: 0.6652 - val_acc: 0.7801\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.5967 - acc: 0.7996 - val_loss: 0.6461 - val_acc: 0.7847\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5971 - acc: 0.8016 - val_loss: 0.6408 - val_acc: 0.7861\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 20s 512us/step - loss: 0.5967 - acc: 0.8020 - val_loss: 0.6330 - val_acc: 0.7934\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5921 - acc: 0.8044 - val_loss: 0.6386 - val_acc: 0.7912\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5948 - acc: 0.8028 - val_loss: 0.6648 - val_acc: 0.7767\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5884 - acc: 0.8056 - val_loss: 0.6349 - val_acc: 0.7899\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5896 - acc: 0.8040 - val_loss: 0.6405 - val_acc: 0.7866\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5806 - acc: 0.8059 - val_loss: 0.6462 - val_acc: 0.7863\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5875 - acc: 0.8063 - val_loss: 0.6419 - val_acc: 0.7848\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5828 - acc: 0.8073 - val_loss: 0.6356 - val_acc: 0.7906\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 21s 519us/step - loss: 0.5833 - acc: 0.8068 - val_loss: 0.6532 - val_acc: 0.7892\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5797 - acc: 0.8073 - val_loss: 0.6337 - val_acc: 0.7945\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 21s 516us/step - loss: 0.5854 - acc: 0.8083 - val_loss: 0.6819 - val_acc: 0.7770\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 21s 515us/step - loss: 0.5743 - acc: 0.8095 - val_loss: 0.6245 - val_acc: 0.7996\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5816 - acc: 0.8081 - val_loss: 0.6494 - val_acc: 0.7955\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 21s 514us/step - loss: 0.5812 - acc: 0.8086 - val_loss: 0.6592 - val_acc: 0.7826\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 20s 506us/step - loss: 0.5823 - acc: 0.8059 - val_loss: 0.6862 - val_acc: 0.7758\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5823 - acc: 0.8083 - val_loss: 0.6340 - val_acc: 0.7961\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 20s 506us/step - loss: 0.5866 - acc: 0.8075 - val_loss: 0.6390 - val_acc: 0.7926\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5786 - acc: 0.8094 - val_loss: 0.6407 - val_acc: 0.7903\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 20s 505us/step - loss: 0.5835 - acc: 0.8077 - val_loss: 0.6400 - val_acc: 0.7968\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5838 - acc: 0.8096 - val_loss: 0.6334 - val_acc: 0.7916\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 20s 504us/step - loss: 0.5807 - acc: 0.8100 - val_loss: 0.6645 - val_acc: 0.7783\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5751 - acc: 0.8125 - val_loss: 0.6535 - val_acc: 0.7873\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5782 - acc: 0.8116 - val_loss: 0.6762 - val_acc: 0.7791\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.5772 - acc: 0.8115 - val_loss: 0.6677 - val_acc: 0.7778\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 20s 510us/step - loss: 0.5791 - acc: 0.8117 - val_loss: 0.6440 - val_acc: 0.7922\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 20s 510us/step - loss: 0.5844 - acc: 0.8104 - val_loss: 0.6322 - val_acc: 0.7931\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5721 - acc: 0.8140 - val_loss: 0.6204 - val_acc: 0.7940\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 20s 505us/step - loss: 0.5836 - acc: 0.8101 - val_loss: 0.6660 - val_acc: 0.7832\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 20s 509us/step - loss: 0.5781 - acc: 0.8113 - val_loss: 0.6119 - val_acc: 0.7968\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 20s 504us/step - loss: 0.5837 - acc: 0.8115 - val_loss: 0.6324 - val_acc: 0.7944\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 20s 504us/step - loss: 0.5773 - acc: 0.8118 - val_loss: 0.6557 - val_acc: 0.7875\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 20s 504us/step - loss: 0.5807 - acc: 0.8108 - val_loss: 0.6307 - val_acc: 0.7937\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5851 - acc: 0.8117 - val_loss: 0.6458 - val_acc: 0.7924\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 20s 504us/step - loss: 0.5820 - acc: 0.8131 - val_loss: 0.7111 - val_acc: 0.7717\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 20s 502us/step - loss: 0.5834 - acc: 0.8117 - val_loss: 0.6611 - val_acc: 0.7878\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 20s 503us/step - loss: 0.5774 - acc: 0.8144 - val_loss: 0.6496 - val_acc: 0.7911\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5867 - acc: 0.8102 - val_loss: 0.6328 - val_acc: 0.7931\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5826 - acc: 0.8138 - val_loss: 0.6465 - val_acc: 0.7903\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5779 - acc: 0.8140 - val_loss: 0.6605 - val_acc: 0.7839\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 21s 517us/step - loss: 0.5851 - acc: 0.8111 - val_loss: 0.6370 - val_acc: 0.7916\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 21s 513us/step - loss: 0.5821 - acc: 0.8143 - val_loss: 0.6503 - val_acc: 0.7890\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5929 - acc: 0.8115 - val_loss: 0.6563 - val_acc: 0.7847\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 20s 510us/step - loss: 0.5879 - acc: 0.8098 - val_loss: 0.6632 - val_acc: 0.7867\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 20s 510us/step - loss: 0.5935 - acc: 0.8111 - val_loss: 0.6865 - val_acc: 0.7806\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5877 - acc: 0.8118 - val_loss: 0.6319 - val_acc: 0.7927\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5888 - acc: 0.8124 - val_loss: 0.6530 - val_acc: 0.7905\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 20s 511us/step - loss: 0.5880 - acc: 0.8088 - val_loss: 0.6594 - val_acc: 0.7884\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5929 - acc: 0.8102 - val_loss: 0.6629 - val_acc: 0.7904\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5968 - acc: 0.8094 - val_loss: 0.6519 - val_acc: 0.7863\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 20s 508us/step - loss: 0.5891 - acc: 0.8105 - val_loss: 0.6425 - val_acc: 0.7906\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 20s 509us/step - loss: 0.5926 - acc: 0.8093 - val_loss: 0.6668 - val_acc: 0.7814\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5962 - acc: 0.8099 - val_loss: 0.6886 - val_acc: 0.7717\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 20s 509us/step - loss: 0.6020 - acc: 0.8099 - val_loss: 0.6689 - val_acc: 0.7829\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 20s 507us/step - loss: 0.5893 - acc: 0.8103 - val_loss: 0.6620 - val_acc: 0.7856\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "AvAe7NSdmcR1",
    "outputId": "62b7ba00-b77d-4ac8-c074-f3512b4035d7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWZwWUf6mcR5"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QgzrXvHmcR6"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Zvdv7coamcR8",
    "outputId": "9b3a7449-b326-493c-c3ae-b4197ae6487a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 0.6569 - acc: 0.7951\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.6579 - acc: 0.7926\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6573 - acc: 0.7923\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6493 - acc: 0.7928\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6531 - acc: 0.7912\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6514 - acc: 0.7927\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6571 - acc: 0.7913\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6618 - acc: 0.7901\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6604 - acc: 0.7902\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6574 - acc: 0.7923\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6551 - acc: 0.7892\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6577 - acc: 0.7907\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6615 - acc: 0.7902\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6622 - acc: 0.7883\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6680 - acc: 0.7914\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6677 - acc: 0.7892\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6572 - acc: 0.7903\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6620 - acc: 0.7895\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6634 - acc: 0.7906\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6685 - acc: 0.7890\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6663 - acc: 0.7905\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6729 - acc: 0.7873\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6733 - acc: 0.7874\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6697 - acc: 0.7881\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6748 - acc: 0.7886\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6826 - acc: 0.7849\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6767 - acc: 0.7878\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.6687 - acc: 0.7887\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6825 - acc: 0.7858\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6832 - acc: 0.7851\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6867 - acc: 0.7829\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6777 - acc: 0.7875\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6796 - acc: 0.7866\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.6883 - acc: 0.7838\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6797 - acc: 0.7858\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.6966 - acc: 0.7837\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6915 - acc: 0.7846\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6933 - acc: 0.7816\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6882 - acc: 0.7839\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.7023 - acc: 0.7820\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6956 - acc: 0.7810\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6919 - acc: 0.7826\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7008 - acc: 0.7801\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6955 - acc: 0.7824\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7053 - acc: 0.7798\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7049 - acc: 0.7781\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7010 - acc: 0.7792\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7112 - acc: 0.7788\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7056 - acc: 0.7793\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7071 - acc: 0.7817\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7113 - acc: 0.7791\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7192 - acc: 0.7803\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.7139 - acc: 0.7790\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.7152 - acc: 0.7791\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7166 - acc: 0.7774\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7082 - acc: 0.7797\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7114 - acc: 0.7791\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7168 - acc: 0.7769\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7175 - acc: 0.7798\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.6984 - acc: 0.7832\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.7150 - acc: 0.7774\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7108 - acc: 0.7768\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.7201 - acc: 0.7760\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7131 - acc: 0.7773\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7191 - acc: 0.7783\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7140 - acc: 0.7780\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.7135 - acc: 0.7796\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.7138 - acc: 0.7779\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7134 - acc: 0.7774\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.7170 - acc: 0.7810\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.7042 - acc: 0.7829\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7161 - acc: 0.7779\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7204 - acc: 0.7776\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.7150 - acc: 0.7765\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7225 - acc: 0.7760\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7163 - acc: 0.7791\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.7232 - acc: 0.7786\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.7285 - acc: 0.7768\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7158 - acc: 0.7773\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.7156 - acc: 0.7789\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7136 - acc: 0.7780\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7117 - acc: 0.7789\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.7111 - acc: 0.7810\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.7083 - acc: 0.7777\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7147 - acc: 0.7783\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.7163 - acc: 0.7786\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7043 - acc: 0.7786\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7077 - acc: 0.7805\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.7085 - acc: 0.7810\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.7061 - acc: 0.7812\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.7124 - acc: 0.7758\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.7048 - acc: 0.7805\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.7084 - acc: 0.7776\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.7133 - acc: 0.7781\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7071 - acc: 0.7796\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.7007 - acc: 0.7833\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.7016 - acc: 0.7827\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.7006 - acc: 0.7814\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.7031 - acc: 0.7820\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.6945 - acc: 0.7834\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGFqNTcNmcR-"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JyX2dCCHmcR_",
    "outputId": "595efe70-e7d3-44b0-d9aa-c100139ca489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 164us/step\n",
      "loss = 0.7267296610832215\n",
      "accuracy = 0.7592\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Jt4x9qQmcSB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HM3_BowenLi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
